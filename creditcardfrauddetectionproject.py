# -*- coding: utf-8 -*-
"""CreditcardfrauddetectionProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QvuCA_A8Fv6WEphQiqO1In1NTXH3nY8b
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

#loading credit card dataset to pandas dataframe
creditcard=pd.read_csv('/content/sample_data/credit.csv')

#printing first 5 records of dataset
creditcard.head(5)

creditcard.tail()

creditcard.info()

# checking number of missing values in a column
creditcard.isnull().sum()

# to handle missing values
import seaborn as sns
from matplotlib import pyplot as plt
#loading the dataset in pandas
creditcard1=pd.read_csv('/content/sample_data/credit.csv')
creditcard1.head()

creditcard1.tail()

#in order to check missing values in a column (revision)
creditcard1.isnull().sum()

# to know rows,columns of a dataset
creditcard1.shape

creditcard1.info()

creditcard1.isnull().sum()

# there are 2 methods to removemissing values
#1.imputation:-Mean.Mode,Median(hundreds and thousands of data)
#2.dropping:-Removing cells or rows containing missing values (lakhs of data)
# analyse the distribution of data in v21
fig,ax = plt.subplots(figsize=(8,8))
sns.distplot(creditcard1.V21)

#replace missing values with median values
creditcard1['V21'].fillna(creditcard1['V21'].median(),inplace=True)
creditcard1.isnull().sum()

fig,ax = plt.subplots(figsize=(8,8))
sns.distplot(creditcard1.V22)

creditcard1['V22'].fillna(creditcard1['V22'].median(),inplace=True)
creditcard1.isnull().sum()

creditcard1.shape

fig,ax = plt.subplots(figsize=(8,8))
sns.distplot(creditcard1.V23)

creditcard1['V23'].fillna(creditcard1['V23'].median(),inplace=True)
creditcard1.isnull().sum()

creditcard1['V24'].fillna(creditcard1['V24'].median(),inplace=True)
creditcard1.isnull().sum()

fig,ax=plt.subplots(figsize=(8,8))
sns.distplot(creditcard1.V25)

creditcard1['V25'].fillna(creditcard1['V25'].median(),inplace=True)
creditcard1.isnull().sum()

#dropping missing values
duplicatecreditdata=pd.read_csv('/content/sample_data/credit.csv')

duplicatecreditdata.shape

duplicatecreditdata.isnull().sum()

#dropping values in a row
duplicatecreditdata=duplicatecreditdata.dropna(how='any')
duplicatecreditdata.isnull().sum()

duplicatecreditdata.shape

#legitimate distribution of normal and fraudulent data
duplicatecreditdata['Class'].value_counts()

"""this dataset is highly inbalanced therefore it needed to be in good state

0-->Normal Transaction

1-->Fraudulent Transaction
"""



#separating the transactions for analysis
legit=duplicatecreditdata[duplicatecreditdata.Class==0]
fraud=duplicatecreditdata[duplicatecreditdata.Class==1]

print(legit.shape)
print(fraud.shape)

#statistical measures of legit variable and frauf variable
legit.Amount.describe()

fraud.Amount.describe()
#the mean and std of fraudulent transactions is greater than normal transactions

# compare the values for both transactions
duplicatecreditdata.groupby('Class').mean()

"""under-sampling to remove unbalanced data

Buid a simple dataset containing similar distribution of legit transaction
and fraudulent transaction

nuumber of fraudulent transactions-->61

in this under sampling, from 14533 normal transactions random 61 transactions will be taken as a data sample and then join it with fraud transaction , so it will have 61 normal transactions and 61 fraud transactions which results to a very good dataset containg normal distribution of normal and fraudulent transaction and distribution is even and can make better perdictions using machine learning.
"""

legit_sample=legit.sample(n=134)

"""to concatenate the two dataframes i.e. legit_sample and fraud
concat function is used and axis parameter when axis= representing rows means the dataframe fraud values will be inserted below dataframe legit_sample values column wise one by one making a new dataframe names new_dataset while axis=1 which represents column will do the same but row wise

"""

new_dataset=pd.concat([legit_sample,fraud],axis=0)

# checking out new dataset consisting random values(random serialization)
new_dataset.head()

new_dataset.tail()

# checking distribution of values in both transactions
new_dataset['Class'].value_counts()

# comparing the values for transactions using groupby
new_dataset.groupby('Class').mean()

"""As we got a great difference between means of different columns so its a bad sample therefore we'll split our data into feature and target

targets-->either 0 or 1
features--> V1,V2,.........V28 and Amount

Splitting data and target
"""

X=new_dataset.drop(columns='Class',axis=1)
Y=new_dataset['Class']

print(X)

print(Y)

"""**split the data into training data and test data**"""

X_train,X_test,Y_train,Y_test=train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=2)

"""X_train,X_test,Y_train,Y_test these are the arrays"""

print(X.shape, X_train.shape, X_test.shape)

"""to train our machine learning model and aceesibility

logistic regression
"""

model=LogisticRegression()

#training our Logistic Regression model with our training data
#fit function is used to fit the features and labels to the model
model.fit(X_train,Y_train)

"""model evaluation and performance

accuracy score
"""

#accuracy on training data
X_trainpredict=model.predict(X_train)
traindataaccuracy=accuracy_score(X_trainpredict, Y_train)

print('ACCURACY SCORE :',traindataaccuracy)

#accuracy on test data
X_testpredict=model.predict(X_test)
testdataaccuracy=accuracy_score(X_testpredict, Y_test)

print('ACCURACY SCORE ON TEST DATA:', testdataaccuracy)

"""if the training data is 97% and the test data is 50% then we can say that test data is overfitted

in underfitting there is very less percentage of training data and very high percentage of test data i.e. vice-versa of overfitting


Hence, my model is neither overfitted nor underfitted.
"""

